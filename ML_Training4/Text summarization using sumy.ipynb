{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "class Summarize_Frequency:\n",
    "  def __init__(self, cut_min=0.2, cut_max=0.8):\n",
    "\t\"\"\"\n",
    "\t Initilize the text summarizer.\n",
    "\t Words that have a frequency term lower than cut_min\n",
    "\t or higer than cut_max will be ignored.\n",
    "\t\"\"\"\n",
    "\tself._cut_min = cut_min\n",
    "\tself._cut_max = cut_max\n",
    "\tself._stopwords = set(stopwords.words('english') + list(punctuation))\n",
    "\n",
    "  def _compute_frequencies(self, word_sent):\n",
    "\t\"\"\"\n",
    "\tCompute the frequency of each of the word\n",
    "\tInput: \n",
    "\t word_sent, a list of sentences already tokenised\n",
    "\tOutput:\n",
    "\t freq, a dictionary where freq[w] is the frequency of where\n",
    "\t\"\"\"\n",
    "\tfreq=defaultdict(int)\n",
    "\tfor s in word_sent:\n",
    "        for word in s:\n",
    "            if word not in self._stopwords:\n",
    "                freq(word) += 1\n",
    "    #Frequency Normalization and filtering\n",
    "    m = float(max(freq.values()))\n",
    "    for w in freq.keys():\n",
    "        freq[w] = freq[w]/m\n",
    "        if freq[w] >= self._cut_max or freq[w] <= self._cut_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Republican memo says the dossier's author, former British intelligence agent Christopher Steele, told a senior justice department official he was \"desperate\" for Mr Trump to lose the White House race.\n",
      "Mr Trump has said the Republican memo, which he declassified on Friday, \"vindicates\" him in the Russia inquiry into whether anyone connected with his campaign colluded with alleged Russian attempts to influence the 2016 US White House race.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"http://www.bbc.com/news/world-us-canada-42950090\"#\"https://en.wikipedia.org/wiki/Principal_component_analysis\"\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "    # or for plain text files\n",
    "    # parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
